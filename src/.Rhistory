library(Sieve)
xdim <- 1
basisN <- 5000
type <- 'sobolev1cos'
TrainData <- Sieve:::GenTrain(s.size = 1000, xdim = xdim, frho = 'additive', frho.para = xdim/2)
TestData <- Sieve:::GenTrain(s.size = 1e3, xdim = xdim, noise.para = 0, frho = 'additive', frho.para = xdim/2)
# index_list <- Sieve::create_index_list(xdim = xdim,basisN = 3e4, interaction_order = 2)
tic('preprocess')
sieve.model <- Sieve::sieve_preprocess(X = TrainData[,2:(xdim+1)], basisN = basisN, type = type,
interaction_order = 2)
library(Rcpp)
library(RcppArmadillo)
library(tictoc)
# index_list <- Sieve::create_index_list(xdim = xdim,basisN = 3e4, interaction_order = 2)
tic('preprocess')
sieve.model <- Sieve::sieve_preprocess(X = TrainData[,2:(xdim+1)], basisN = basisN, type = type,
interaction_order = 2)
toc()
sieve.model$X
dim(sieve.model$Phi)
tic('solve the lasso problem')
sieve.model <- Sieve::sieve_solver(sieve.model, TrainData$Y, l1 = TRUE)
toc()
sieve_model_prediction <- Sieve::sieve_predict(testX = TestData[,2:(xdim+1)], testY = TestData$Y, sieve.model)
library(Rcpp)
library(RcppArmadillo)
library(tictoc)
setwd('/Users/tianyu/Documents/SievePackage/Sieve/src/')
# sourceCpp("PracticeRcppFunction.cpp")
sourceCpp('C_Functions.cpp')
source('/Users/tianyu/Documents/SievePackage/Sieve/R/SieveFittingModels.R')
sieve_predict(testX = X[4,1:2], testY = X[4,]$Y, sieve.model)
X <- data.frame(x1 = as.factor(c('a','b','c','c','b')), x2 = c(1,8,3,6,6), Y = c(1,2,3,3,2))
sieve.model <- sieve_preprocess(X = X[1:3,1:2], basisN = 1000, type = 'sobolev1cos',
interaction_order = 3)
TrainData <- Sieve:::GenTrain(s.size = 1000, xdim = xdim, frho = 'additive', frho.para = xdim/2)
TestData <- Sieve:::GenTrain(s.size = 1e3, xdim = xdim, noise.para = 0, frho = 'additive', frho.para = xdim/2)
# index_list <- Sieve::create_index_list(xdim = xdim,basisN = 3e4, interaction_order = 2)
tic('preprocess')
sieve.model <- Sieve::sieve_preprocess(X = TrainData[,2:(xdim+1)], basisN = basisN, type = type,
interaction_order = 2)
toc()
tic('solve the lasso problem')
sieve.model <- Sieve::sieve_solver(sieve.model, TrainData$Y, l1 = TRUE)
toc()
sieve_model_prediction <- Sieve::sieve_predict(testX = TestData[,2:(xdim+1)], testY = TestData$Y, sieve.model)
matirx(0, nrow = NULL, ncol = 100)
matrix(0, nrow = NULL, ncol = 100)
#'
#' Use the fitted sieve regression model from sieve_solver. It also returns the testing mean-squared errors.
#'
#' @param model a list. Use the fitted model from sieve_solver.
#' @param testX a data frame. Dimension equals to test sample size x feature diemnsion. Should be of a similar format as the training feature provided to sieve_preprocess.
#' @param testY a vector. The outcome of testing samples (if known). Default is NULL. For regression problems, the algorithm also returns the testing mean-squared errors.
#'
#' @return A vector of n colour hex codes
#' @export
#'
sieve_predict <- function(model, testX, testY = NULL){
if(!is.null(model$lambda)){
lambda_num <- length(model$lambda)
beta_hat <- matrix(model$beta_hat, ncol = lambda_num)
}else{
lambda_num <- 1
beta_hat <- model$beta_hat
}
type <- model$type
basisN <- model$basisN
index_matrix <- model$index_matrix
norm_para <- model$norm_para #specifies how to normalize the testing features/predictors.
if(class(testX) == 'numeric'){
test.size <- length(testX)
xdim <- 1
}else{
test.size <- dim(testX)[1]
xdim <- dim(testX)[2]
}
# testX <- as.matrix(testX, nrow = test.size)
test_list <- sieve_preprocess(X = testX, basisN = basisN,
type = type, index_matrix = index_matrix,
norm_para = norm_para) #provide index_matrix
print(test.size)
print(lambda_num)
predictY <- matrix(0, nrow = test.size, ncol = lambda_num)
test_Phi <- test_list$Phi
if(!is.null(testY)){
MSE <- rep(0, lambda_num)
for(i in 1:lambda_num){
predictY[,i] <- crossprod_C(test_Phi, matrix(beta_hat[,i]))
MSE[i] <- mean((predictY[,i] - testY)^2)
}
return(list(predictY = predictY, MSE = MSE))
}else{
for(i in 1:lambda_num){
predictY[,i] <- crossprod_C(test_Phi, matrix(beta_hat[,i]))
}
return(list(predictY = predictY))
}
}
sieve_model_prediction <- Sieve::sieve_predict(testX = TestData[,2:(xdim+1)], testY = TestData$Y, sieve.model)
#'
#' Use the fitted sieve regression model from sieve_solver. It also returns the testing mean-squared errors.
#'
#' @param model a list. Use the fitted model from sieve_solver.
#' @param testX a data frame. Dimension equals to test sample size x feature diemnsion. Should be of a similar format as the training feature provided to sieve_preprocess.
#' @param testY a vector. The outcome of testing samples (if known). Default is NULL. For regression problems, the algorithm also returns the testing mean-squared errors.
#'
#' @return A vector of n colour hex codes
#' @export
#'
sieve_predict <- function(model, testX, testY = NULL){
if(!is.null(model$lambda)){
lambda_num <- length(model$lambda)
beta_hat <- matrix(model$beta_hat, ncol = lambda_num)
}else{
lambda_num <- 1
beta_hat <- model$beta_hat
}
type <- model$type
basisN <- model$basisN
index_matrix <- model$index_matrix
norm_para <- model$norm_para #specifies how to normalize the testing features/predictors.
if(class(testX) == 'numeric'){
test.size <- length(testX)
xdim <- 1
}else{
test.size <- dim(testX)[1]
xdim <- dim(testX)[2]
}
# testX <- as.matrix(testX, nrow = test.size)
print(test.size)
print(lambda_num)
test_list <- sieve_preprocess(X = testX, basisN = basisN,
type = type, index_matrix = index_matrix,
norm_para = norm_para) #provide index_matrix
print(test.size)
print(lambda_num)
predictY <- matrix(0, nrow = test.size, ncol = lambda_num)
test_Phi <- test_list$Phi
if(!is.null(testY)){
MSE <- rep(0, lambda_num)
for(i in 1:lambda_num){
predictY[,i] <- crossprod_C(test_Phi, matrix(beta_hat[,i]))
MSE[i] <- mean((predictY[,i] - testY)^2)
}
return(list(predictY = predictY, MSE = MSE))
}else{
for(i in 1:lambda_num){
predictY[,i] <- crossprod_C(test_Phi, matrix(beta_hat[,i]))
}
return(list(predictY = predictY))
}
}
sieve_model_prediction <- sieve_predict(testX = TestData[,2:(xdim+1)], testY = TestData$Y, sieve.model)
